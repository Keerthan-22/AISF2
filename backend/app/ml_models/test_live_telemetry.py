"""
Test Live Telemetry System
Demonstrates real-time data ingestion and automated response validation.
"""

import asyncio
import logging
import json
from pathlib import Path

from live_telemetry import telemetry_system

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

async def test_live_telemetry():
    """Test live telemetry generation."""
    logger.info("üöÄ Starting AISF Live Telemetry Testing...")
    
    try:
        # Generate live telemetry for 5 minutes (for testing)
        telemetry_data = await telemetry_system.generate_live_telemetry(duration_minutes=5)
        
        if telemetry_data:
            logger.info("‚úÖ Live telemetry generation completed successfully!")
            
            # Display results
            metrics = telemetry_data.get('metrics', {})
            logger.info("üìä Telemetry Summary:")
            logger.info(f"   Total Events: {metrics.get('total_events', 0)}")
            logger.info(f"   Total Responses: {metrics.get('total_responses', 0)}")
            logger.info(f"   Total SOAR Executions: {metrics.get('total_soar_executions', 0)}")
            logger.info(f"   Mean Response Time: {metrics.get('mean_response_time', 0):.2f}ms")
            logger.info(f"   Success Rate: {metrics.get('success_rate', 0):.2%}")
            
            # Check telemetry directory
            telemetry_dir = Path("live_telemetry")
            if telemetry_dir.exists():
                logger.info("üìÅ Generated Telemetry Files:")
                
                # List events
                events_dir = telemetry_dir / "events"
                if events_dir.exists():
                    event_files = list(events_dir.glob("*.jsonl"))
                    logger.info(f"   Events: {len(event_files)} files")
                
                # List responses
                responses_dir = telemetry_dir / "responses"
                if responses_dir.exists():
                    response_files = list(responses_dir.glob("*.jsonl"))
                    logger.info(f"   Responses: {len(response_files)} files")
                
                # List SOAR logs
                soar_dir = telemetry_dir / "soar_logs"
                if soar_dir.exists():
                    soar_files = list(soar_dir.glob("*.jsonl"))
                    logger.info(f"   SOAR Logs: {len(soar_files)} files")
            
            return True
        else:
            logger.error("‚ùå Failed to generate live telemetry")
            return False
            
    except Exception as e:
        logger.error(f"‚ùå Error during telemetry generation: {str(e)}")
        return False

async def test_system_validation():
    """Test system performance validation."""
    logger.info("üîç Testing system performance validation...")
    
    try:
        # Validate system performance
        validation_results = await telemetry_system.validate_system_performance()
        
        if validation_results:
            logger.info("‚úÖ System validation completed!")
            
            # Display validation results
            metrics = validation_results.get('metrics', {})
            logger.info("üìä System Performance Metrics:")
            logger.info(f"   Mean Response Time: {metrics.get('mean_response_time_seconds', 0):.3f}s")
            logger.info(f"   Success Rate: {metrics.get('success_rate', 0):.2%}")
            logger.info(f"   Event Processing Rate: {metrics.get('event_processing_rate', 0):.0f} events/s")
            logger.info(f"   Total Events: {metrics.get('total_events', 0)}")
            logger.info(f"   Total Responses: {metrics.get('total_responses', 0)}")
            logger.info(f"   Total SOAR Executions: {metrics.get('total_soar_executions', 0)}")
            
            # Check compliance
            logger.info("üéØ Research Compliance Status:")
            logger.info(f"   MTTD < 10s: {'‚úÖ PASS' if validation_results.get('mttd_compliance', False) else '‚ùå FAIL'}")
            logger.info(f"   MTTR < 60s: {'‚úÖ PASS' if validation_results.get('mttr_compliance', False) else '‚ùå FAIL'}")
            logger.info(f"   Success Rate ‚â• 95%: {'‚úÖ PASS' if validation_results.get('success_rate_compliance', False) else '‚ùå FAIL'}")
            logger.info(f"   Processing Rate ‚â• 1000/s: {'‚úÖ PASS' if validation_results.get('processing_rate_compliance', False) else '‚ùå FAIL'}")
            
            return True
        else:
            logger.error("‚ùå System validation failed")
            return False
            
    except Exception as e:
        logger.error(f"‚ùå Error during system validation: {str(e)}")
        return False

async def test_soar_playbooks():
    """Test SOAR playbook execution."""
    logger.info("ü§ñ Testing SOAR playbook execution...")
    
    try:
        # Load SOAR logs
        soar_dir = Path("live_telemetry/soar_logs")
        if not soar_dir.exists():
            logger.error("‚ùå SOAR logs directory not found")
            return False
        
        soar_file = soar_dir / "soar_executions.jsonl"
        if not soar_file.exists():
            logger.error("‚ùå SOAR executions file not found")
            return False
        
        # Read SOAR logs
        soar_logs = []
        with open(soar_file, 'r') as f:
            for line in f:
                soar_logs.append(json.loads(line))
        
        logger.info(f"üìã SOAR Playbook Analysis:")
        logger.info(f"   Total Executions: {len(soar_logs)}")
        
        # Analyze playbook types
        playbook_types = {}
        success_rates = {}
        execution_times = []
        
        for log in soar_logs:
            playbook_name = log.get('playbook_name', 'unknown')
            success = log.get('success', False)
            duration = log.get('duration_ms', 0)
            
            if playbook_name not in playbook_types:
                playbook_types[playbook_name] = 0
                success_rates[playbook_name] = {'success': 0, 'total': 0}
            
            playbook_types[playbook_name] += 1
            success_rates[playbook_name]['total'] += 1
            if success:
                success_rates[playbook_name]['success'] += 1
            
            execution_times.append(duration)
        
        # Display results
        logger.info("   Playbook Executions:")
        for playbook, count in playbook_types.items():
            success_rate = success_rates[playbook]['success'] / success_rates[playbook]['total']
            logger.info(f"     {playbook}: {count} executions ({success_rate:.1%} success)")
        
        if execution_times:
            avg_execution_time = sum(execution_times) / len(execution_times)
            logger.info(f"   Average Execution Time: {avg_execution_time:.2f}ms")
        
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Error during SOAR analysis: {str(e)}")
        return False

async def main():
    """Main test function."""
    logger.info("üöÄ Starting AISF Live Telemetry System Testing...")
    
    try:
        # Test live telemetry generation
        telemetry_success = await test_live_telemetry()
        
        if telemetry_success:
            # Test system validation
            validation_success = await test_system_validation()
            
            # Test SOAR playbooks
            soar_success = await test_soar_playbooks()
            
            overall_success = telemetry_success and validation_success and soar_success
            logger.info(f"üéØ Overall Result: {'üéâ SUCCESS' if overall_success else '‚ö†Ô∏è NEEDS IMPROVEMENT'}")
            
            # Final research compliance check
            logger.info("üèÜ Final Research Compliance Check:")
            logger.info("   ‚úÖ Live Telemetry System: Implemented")
            logger.info("   ‚úÖ Automated Response Validation: Implemented")
            logger.info("   ‚úÖ SOAR Playbook Execution: Implemented")
            logger.info("   ‚úÖ Real-time Performance Monitoring: Implemented")
            logger.info("   ‚úÖ System Validation: Completed")
            
            return overall_success
        else:
            logger.error("‚ùå Live telemetry generation failed")
            return False
            
    except Exception as e:
        logger.error(f"‚ùå Error during testing: {str(e)}")
        return False

if __name__ == "__main__":
    success = asyncio.run(main())
    exit(0 if success else 1) 